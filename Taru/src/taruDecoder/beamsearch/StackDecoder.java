package taruDecoder.beamsearch;

import java.util.*;

import taruDecoder.Decoder;
import taruDecoder.Scorer;
import taruHypothesis.Hypothesis;
import tarugrammar.*;
import treeParser.*;
 
public class StackDecoder implements Decoder {

	/* * BeamSearch based stack decoder
	 * 
	 * Takes care of insertions to create the targetForest -
	 *   Could insert into Hypergraph for CubeGrowing and CubePruning decoding 
	 */

	//TODO: Implement thresholds for both rule and lexical beams 
	private int RULE_BEAM = 10;
	private int LEXICAL_BEAM = 10;
	private int PHRASE_BEAM = 10;

	// Beams 
	public List<Beam> beamArr = null;
	// Hashmaps to enable packing of the forest. A matching node is keyed on type and source span it covers. A inserted node or a lexical node is keyed on the lexical contents
	public HashMap<String, Integer> matchingNodes;

	// These are the partial sequence of lexical items found in RULES. They are packed for efficient computation using the LM ;Keyed on the phrase itself
	public HashMap<String, Integer> insertedNodes;


	public StackDecoder(){  
			// Initialization (number of nodes in the tree)
			beamArr = new ArrayList<Beam>(10);
			int N = 10;
			for(int i=0;i<N;i++){
				beamArr.add(new Beam(i));
			}
			// beamArr(N).addHypothesis(new Hypothesis("TOP"));
			
			matchingNodes = new HashMap<String, Integer>();
			insertedNodes = new HashMap<String, Integer>();
	}
	
 	
		// Create a hypothesis by combining the missing pieces in it 
		// They should have already been created from the earlier beams 
		public void hypothesisCombination(){
		}
	 
		public List<Hypothesis> extract_phase(int N)
		{
			//Scorer.getScorer().setHyperGraph(binHG);
			// Extracts and sets the target hypothesis for the TOP node
			// HGUtils.extractKBestHypothesis(binHG, 100);
		
			return beamArr.get(beamArr.size()).getTopKHypothesis(N);
		}
		
		public void handleOOV(ParseTreeNode ptn) {
			// Now add a hypothesis just copying over the source word.
			int[] kindex = new int[2];
			kindex[0] = kindex[1] = -1;
			Hypothesis hyp = new Hypothesis(ptn.getS(),ptn.getS(), -1, kindex);
			Scorer.getScorer().initializeHypothesisFeatures(hyp);
 
			// First find all the target side nodes generated by this source node.
			//for (String nodeType : ptn.targetNodeTypes) {
				beamArr.get(ptn.id).addHypothesis(hyp);
			//}
		}

		public void addLexicalBackoff(LexicalRule backoff, ParseTreeNode ptn){
			// Now add a hypothesis just copying over the source word.
			int[] kindex = new int[2];
			kindex[0] = kindex[1] = -1;
			Hypothesis hyp = new Hypothesis(backoff.getTargetWord(),ptn.getS(), -1, kindex);
			Scorer.getScorer().initializeHypothesisFeatures(hyp);
 
			// First find all the target side nodes generated by this source node.
			// for (String nodeType : ptn.targetNodeTypes) {
	 		//	System.err.println("Lexical backoff handling - Adding to " + nodeType);
				beamArr.get(ptn.id).addHypothesis(hyp);
			//}
		}

		public boolean processMatch(ParseTreeNode ptn, Integer patId, ArrayList<ParseTreeNode> frontiers,
						ArrayList<GrammarRule> targetStrings) {

			boolean result = false;
			for (GrammarRule targetInfo : targetStrings) {
				result = addToTargetForest(targetInfo, frontiers, ptn.isRoot) || result;
			}
			return result;
		}

		public void addGlueRulesToTargetForest(ParseTreeNode ptn) {

			// Create a Glue node 
			String goalkey = ptn.spanString+":X";
			
			// Monotonically string the children as ITEMs (They are already created)
			int [] kindex = new int[2];
			kindex[0] = kindex[1] = -1;			 
			String targetglue ="";String sourceglue = "";
			for (int i = 0; i < ptn.children.size(); i++) {
				ParseTreeNode child = ptn.children.elementAt(i);
				String nodeKey = child.spanString + ":"+child.nodetype;
				targetglue+=nodeKey+" ";
			}
			Hypothesis hyp = new Hypothesis(targetglue,sourceglue,-1,kindex);
			beamArr.get(ptn.id).addHypothesis(hyp);
		}

		// Adding a syntactic phrase, (SPMT) style, instead of actual phrases
		public void addPhraseEntriesToForest(ArrayList<PhraseRule> targetList, ParseTreeNode frontier, boolean isRoot){
 			for(PhraseRule prule : targetList){
				// Create the hyp
				int [] kindex = new int[2];
				kindex[0] = kindex[1] = -1;
				Hypothesis hyp = new Hypothesis(prule.getTargetWord(), prule.getSourceWord(), -1, kindex);
				// [TODO] This is a hack to get things working quickly
				Scorer.getScorer().initializeHypothesisFeatures(hyp,prule);
				beamArr.get(frontier.id).addHypothesis(hyp);
			}
		}

		public boolean addToTargetForest(GrammarRule targetInfo, ArrayList<ParseTreeNode> frontiers, boolean isSrcRoot) {
			String targetStr = targetInfo.getTarget();
			// 0:NP to come 1:NP 2:VP
			String[] tokens = targetStr.split(" +");
			// StringTokenizer tok = new StringTokenizer(targetStr,"\\s+");
			// NP get the goal information
			//String[] tmp = tokens[0].split(":");

			int [] kindex = new int[2];
			kindex[0] = kindex[1] = -1;
			Hypothesis hyp = new Hypothesis(targetStr, "", -1, kindex);
				if (frontiers.size() > 1) {
					// This is a dummy node, only wc,LM feature is active here
					Scorer.getScorer().initializeHypothesisFeatures(hyp,1);
				} else {
					Scorer.getScorer().initializeHypothesisFeatures(hyp, targetInfo);
				}
		// The first frontier is the root
		beamArr.get(frontiers.get(0).id).addHypothesis(hyp);
		return true;
		}
}
